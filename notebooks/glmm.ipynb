{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d59a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-07 11:27:09,600 - kedro.io.data_catalog - INFO - Loading data from `orders_full` (PickleDataSet)...\n",
      "2022-03-07 11:27:09,961 - kedro.io.data_catalog - INFO - Loading data from `enc_frame` (PickleDataSet)...\n",
      "2022-03-07 11:27:11,079 - kedro.io.data_catalog - INFO - Loading data from `orders_features` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "orders = catalog.load(\"orders_full\")\n",
    " # orders_features = catalog.load(\"orders_features\")\n",
    "enc_frame = catalog.load(\"enc_frame\")\n",
    "\n",
    "orders[\"basketID\"] = orders[\"customerID\"].astype(str) + orders[\"orderDate\"].astype(str)\n",
    "train = orders[orders[\"val_set\"]==0]\n",
    "orders_features = catalog.load(\"orders_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3762b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50078"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(orders_features[\"val_set\"]==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fbf98ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09430405610260872"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50078/orders_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bohb_args = parameters[\"bohb_catboost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2fd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parser\n",
    "import argparse\n",
    "def create_args(description, min_budget, max_budget, n_iterations, n_workers):\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.', default=min_budget) #9\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.', default=max_budget)  #243\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=n_iterations) #5\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=n_workers)    #32\n",
    "    parser.add_argument('--run_id', type=str, help='A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.')\n",
    "    args=parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f272e7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--run_id'], dest='run_id', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.', metavar=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--run_id', type=str, help='A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee88c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-07 10:57:49,466 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...\n"
     ]
    }
   ],
   "source": [
    "parameters = catalog.load(\"parameters\")\n",
    "enc_cols = catalog.load(\"enc_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42001d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pass=['weekdayOrdered', 'weekdayReceived', 'weDelivery', 'accOldSys', \"color\", \"size\", \"userAgeImp\"]\n",
    "# double: one normal + one catboost col; seperate for arguments -> [\"manufacturerID\", \"salutation\", \"state\"] + ['item_color_cat', 'item_size_cat']\n",
    "cat_catboost=[\"manufacturerID\", \"salutation\", \"state\", 'item_color_cat', 'item_size_cat', 'monthOrdered', \"itemID\"] # \n",
    "cont=['discount', 'basket_price', 'basketNArts', 'durationDelivery', 'accAgeAtOrder', \"UserAgeOrder\"]\n",
    "o_feat_cols = orders_features.columns.to_series().iloc[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81174b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdapting to dmc dataset: repair n and N (on creation)\\n(1) groupby baskets_duplicated.sum -> depending on no of joint arts\\nAdapt basket\\n(2) drop article (self) once [np.where(a==1)[0][0]]\\n(3) drop duplicated data from basket --> unique function\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapting to dmc dataset: repair n and N (on creation)\n",
    "(1) groupby baskets_duplicated.sum -> depending on no of joint arts\n",
    "Adapt basket\n",
    "(2) drop article (self) once [np.where(a==1)[0][0]]\n",
    "(3) drop duplicated data from basket --> unique function\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) drop article (self) once [np.where(a==1)[0][0]]\n",
    "# (3) drop duplicated data from basket --> unique function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9aa9fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_counts.apply(lambda x: pd.Series(data=x, index=n.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4a3616da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basketID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>returnShipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451942</th>\n",
       "      <td>87102013-03-16</td>\n",
       "      <td>2603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451943</th>\n",
       "      <td>87102013-03-16</td>\n",
       "      <td>3064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              basketID  itemID  returnShipment\n",
       "451942  87102013-03-16    2603               0\n",
       "451943  87102013-03-16    3064               1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dff[dff[meta_col].isin(data_meta.loc[3064])]\n",
    "# data_meta.loc[3065]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3460830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff[dff[\"itemID\"]==1][\"returnShipment\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "977df7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "740120da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.tail(20)\n",
    "# dff[dff[data_col]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "c56331d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_ret.loc[1].drop(drops.loc[1].index, errors='ignore').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "e58927a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops.loc[inc].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e59b2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_ret.groupby(\"itemID\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6745ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most nas originate from 64 arts\n",
    "# misses columns in n if article was not in basket with return: impute from N\n",
    "N_cols = N.columns.to_series()\n",
    "non_n_df = pd.DataFrame(columns= N_cols[~N_cols.isin(n.columns)], index = N.index)\n",
    "n = pd.merge(n, non_n_df, left_index=True, right_index = True)\n",
    "# n is not symmetrical: it depends on which articles get returned together\n",
    "n = n[N.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "c099d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff[dff[meta_col].isin(np.intersect1d(data_meta[1], data_meta[5]))][\"itemID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fe0a372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enc_str = BayLOO2d(variants=True)\\norders = catalog.load(\"orders_full\")\\norders_features = catalog.load(\"orders_features\")\\n\\norders[\"basketID\"] = orders[\"customerID\"].astype(str) + orders[\"orderDate\"].astype(str)\\ntrain = orders[orders[\"val_set\"]==0]\\nenc_str.fit(train[\"itemID\"].astype(str), train[\"returnShipment\"], train[\"basketID\"])\\nenc_train_str = enc_str.transform(data = train[\"itemID\"].astype(str), meta_group = train[\"basketID\"], targets=train[\"returnShipment\"])'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"enc_str = BayLOO2d(variants=True)\n",
    "orders = catalog.load(\"orders_full\")\n",
    "orders_features = catalog.load(\"orders_features\")\n",
    "\n",
    "orders[\"basketID\"] = orders[\"customerID\"].astype(str) + orders[\"orderDate\"].astype(str)\n",
    "train = orders[orders[\"val_set\"]==0]\n",
    "enc_str.fit(train[\"itemID\"].astype(str), train[\"returnShipment\"], train[\"basketID\"])\n",
    "enc_train_str = enc_str.transform(data = train[\"itemID\"].astype(str), meta_group = train[\"basketID\"], targets=train[\"returnShipment\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fe105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arts_ij_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "13484ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iid in test[\"itemID\"][~test[\"itemID\"].isin(train[\"itemID\"])]:\n",
    "#    print((train[\"itemID\"]==iid).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9edb2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-05 21:19:12,072 - kedro.io.data_catalog - INFO - Loading data from `orders_full` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "enc = BayLOO2d(variants=True)\n",
    "orders = catalog.load(\"orders_full\")\n",
    "# orders_features = catalog.load(\"orders_features\")\n",
    "\n",
    "orders[\"basketID\"] = orders[\"customerID\"].astype(str) + orders[\"orderDate\"].astype(str)\n",
    "train = orders[orders[\"val_set\"]==0]\n",
    "test = orders[orders[\"val_set\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77033e3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_964/736780453.py:165: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BayLOO2d at 0x7f8269f90ca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(train[\"itemID\"], train[\"returnShipment\"], train[\"basketID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f35c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = enc.transform(data =train[\"itemID\"], meta_group = train[\"basketID\"], targets = train[\"returnShipment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6915eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_test = enc.transform(data =test[\"itemID\"], meta_group = test[\"basketID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e9317d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(enc.arts_ij_posterior[\"test\"]).apply(lambda x: x.isna().sum(), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ebcb7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_t = enc.transform(data =train[\"itemID\"], meta_group = train[\"basketID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d0f4be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_train_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43menc_train_t\u001b[49m\u001b[38;5;241m.\u001b[39mloc[train[train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturnShipment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m      2\u001b[0m enc_train_t\u001b[38;5;241m.\u001b[39mloc[train[train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturnShipment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mindex]\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_train_t' is not defined"
     ]
    }
   ],
   "source": [
    "print(enc_train_t.loc[train[train[\"returnShipment\"]==1].index].mean(),\n",
    "enc_train_t.loc[train[train[\"returnShipment\"]==0].index].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e29e4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5400790719927719 0.4660672961054185\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.loc[train[train[\"returnShipment\"]==1].index].mean(),\n",
    "enc_train.loc[train[train[\"returnShipment\"]==0].index].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f769aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5715980619044962 0.49814729007135694\n"
     ]
    }
   ],
   "source": [
    "print(enc_test[test[test[\"returnShipment\"]==1].index].mean(),\n",
    "      enc_test[test[test[\"returnShipment\"]==0].index].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7a1660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5198234935321024 0.448691804286501 \n",
    "0.546005061750093 0.48251791860307236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "565e4b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pd.DataFrame(enc.arts_ij_posterior[\"train_f\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()\\npd.DataFrame(enc.arts_ij_posterior[\"train_t\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()\\npd.DataFrame(enc.arts_ij_posterior[\"test\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"pd.DataFrame(enc.arts_ij_posterior[\"train_f\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()\n",
    "pd.DataFrame(enc.arts_ij_posterior[\"train_t\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()\n",
    "pd.DataFrame(enc.arts_ij_posterior[\"test\"]).apply(lambda x: x.isna().sum(), axis=1).value_counts()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "2dc29a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in enc.arts_ij_posterior.keys():\n",
    "    enc.arts_ij_posterior[key] = np.nan_to_num(enc.arts_ij_posterior[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ec41b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders[orders[\"val_set\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "5f8cd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[enc_train[enc_train.isna()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "2b20eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train[\"basketID\"].isin(train.loc[enc_train.loc[enc_train.isna()].index][\"basketID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5ff8d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_ret.loc[train[train[\"returnShipment\"]==1].index].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d544e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examp = enc.transform(train[\"itemID\"], train[\"returnShipment\"], train[\"basketID\"])\n",
    "# enc.arts_posterior['train_f']\n",
    "# 178/273\n",
    "# a = dff[(dff[\"itemID\"]==1)]\n",
    "# a[\"basketID\"].value_counts()[a[\"basketID\"].value_counts()>1]\n",
    "# a[a[meta_col].duplicated(keep=False)]#[\"returnShipment\"]\n",
    "# data_meta.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5b6d6",
   "metadata": {},
   "source": [
    "### Probleme\n",
    "* einzelarts mit only-return schmeissen nuller (64 -> na in df)\n",
    "    * in einzelbestellung oder nicht?\n",
    "* 7 Artikel, die nur einmal mit sich selbst bestellt wurden\n",
    "    * sind na -> ?\n",
    "    * drop: nicht nutzen wenn mit sich selbst bestellt wurde\n",
    "* na due to inf -> replace them with np.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "484f10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_N = grouped_N.groupby(data_col).sum()\n",
    "# drops = alle arts die \n",
    "drops = grouped[grouped==1]\n",
    "grouped_ret = dff_rets.groupby(data_col)[meta_col].value_counts()\n",
    "drop_inc = drops.index.get_level_values(0).unique().to_series()\n",
    "ret_inc = grouped_ret.index.get_level_values(0).unique().to_series()\n",
    "# drop_inc: only arts that got returned (grouped_ret)\n",
    "drop_inc = drop_inc[drop_inc.isin(ret_inc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# problem -> get \n",
    "for art in meta_arts:\n",
    "    n.loc[art, art] = 0\n",
    "    N.loc[art,art] = 0\n",
    "grouped = dff.groupby(data_col)[meta_col].value_counts()\n",
    "grouped_N = grouped[grouped>1]\n",
    "\n",
    "variants_N = grouped_N.groupby(data_col).sum()\n",
    "# drops = alle arts die \n",
    "drops = grouped[grouped==1]\n",
    "grouped_ret = dff_rets.groupby(data_col)[meta_col].value_counts()\n",
    "drop_inc = drops.index.get_level_values(0).unique().to_series()\n",
    "ret_inc = grouped_ret.index.get_level_values(0).unique().to_series()\n",
    "# drop_inc: only arts that got returned (grouped_ret)\n",
    "drop_inc = drop_inc[drop_inc.isin(ret_inc)]\n",
    "for inc in drop_inc:\n",
    "    # if grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore'):\n",
    "    n.loc[inc,inc] = grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore').sum()        \n",
    "for item in variants_N.index:\n",
    "    N.loc[item, item] = variants_N.loc[item]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "5467846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#if self.variants == True:\n",
    "uniqueframe = dff[[meta_col, data_col]].drop_duplicates()\n",
    "meta_arts_new = np.sort(uniqueframe[uniqueframe[meta_col].isin(meta_multi) &\n",
    "(uniqueframe[data_col].duplicated(keep=False))][data_col].unique())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f30247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[enc_train[enc_train.isna()].index, \"itemID\"].isin(meta_arts_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24eba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train[\"basketID\"].isin([\"56982012-04-08\", \"45532012-04-09\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e7c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.meta_arts.loc[626]\n",
    "# np.sort(ind_met)[619]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "9e976116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_met = enc.meta_arts[enc.meta_arts.isin(meta_arts_new)].index.to_numpy()\n",
    "frame = enc.arts_ij_posterior[\"test\"][np.sort(ind_met), :]\n",
    "middle = frame[:, np.sort(ind_met)]\n",
    "pd.DataFrame(middle).loc[619, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "128e6c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619    True\n",
       "Name: 0, dtype: bool"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(middle)[0].isna()[pd.DataFrame(middle)[0].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418dec1",
   "metadata": {},
   "source": [
    "## work area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9748c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[\"itemID\"]\n",
    "targets = train[\"returnShipment\"]\n",
    "meta_group = train[\"basketID\"]\n",
    "# get colnames (using pandas .name attribute -> needed atm)\n",
    "data_col = data.name\n",
    "target_col = targets.name\n",
    "meta_col = meta_group.name\n",
    "\n",
    "# generate data used for aricle-wise stage\n",
    "prior_mean = targets.mean() # mean of dataset\n",
    "# self.prior_mean = prior_mean\n",
    "\n",
    "df = pd.merge(data, targets, left_index = True, right_index = True)\n",
    "\n",
    "# first stage: equal to (leave one out) beta encoder\n",
    "stats = df.groupby(data_col)\n",
    "stats = stats.agg(['sum', 'count'])[target_col]\n",
    "stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "stats = stats.reset_index(level=0).set_index(data_col)\n",
    "# generate arts_lookup for single-arts\n",
    "stats_f = stats.copy()\n",
    "stats_f.loc[:, \"N\"] = stats_f[\"N\"] - 1\n",
    "# stats_f.loc[stats_f[\"N\"]>1, \"N\"] = stats_f[stats_f[\"N\"]>1][\"N\"] - 1\n",
    "stats_t = stats_f.copy()\n",
    "stats_t.loc[stats_t[\"n\"]>0, \"n\"] = stats_f[\"n\"] - 1\n",
    "\n",
    "# first stage: article-meta stage for article-basket data\n",
    "dff = pd.merge(meta_group, df, left_index = True, right_index = True)\n",
    "\n",
    "meta_data = dff.groupby(meta_col)[data_col].apply(np.array)\n",
    "meta_data = meta_data.apply(np.unique)\n",
    "\n",
    "meta_lens = meta_data.apply(lambda x: len(x))\n",
    "meta_multi = meta_lens[meta_lens>1].index.to_series()\n",
    "#if self.variants == True:\n",
    "#    uniqueframe = dff[[meta_col, data_col]].drop_duplicates()\n",
    "#    meta_arts = np.sort(uniqueframe[uniqueframe[meta_col].isin(meta_multi) &\n",
    "#                        (uniqueframe[data_col].duplicated(keep=False))][data_col].unique())\n",
    "#else:\n",
    "meta_arts = np.sort(dff[dff[meta_col].isin(meta_multi)][data_col].unique())\n",
    "# self.meta_arts = pd.Series(meta_arts, name=data_col)\n",
    "\n",
    "dff = dff[dff[meta_col].isin(meta_multi)] # (dff[data_col].isin(meta_arts))\n",
    "dff_rets = dff[dff[target_col]==1]\n",
    "\n",
    "data_meta = dff.groupby(data_col)[meta_col].apply(np.array)\n",
    "data_meta = data_meta.apply(lambda x: np.unique(x)) # unique: dont take multi-arts in same basket into account\n",
    "data_ret_meta = dff_rets.groupby(data_col)[meta_col].apply(np.array)\n",
    "data_ret_meta = data_ret_meta.apply(lambda x: np.unique(x))\n",
    "\n",
    "joint_datapoints_N = data_meta.apply(lambda x: np.concatenate(meta_data.loc[x].to_numpy())) \n",
    "joint_datapoints_n = data_ret_meta.apply(lambda x: np.concatenate(meta_data.loc[x].to_numpy()))\n",
    "\n",
    "counted_N = joint_datapoints_N.apply(lambda x: np.unique(x, return_counts=True))\n",
    "counted_n = joint_datapoints_n.apply(lambda x: np.unique(x, return_counts=True))\n",
    "\n",
    "N = counted_N.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "n = counted_n.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "# N = N.T # transform to apply index first\n",
    "# n = n.T\n",
    "N = N.loc[meta_arts, meta_arts]\n",
    "n = n.reindex(meta_arts, axis=1).reindex(meta_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b906a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_joint_arts(art1, art2):\n",
    "    best1 = train[train[\"itemID\"]==art1][\"basketID\"].unique()\n",
    "    best2 = train[train[\"itemID\"]==art2][\"basketID\"].unique()\n",
    "    return pd.Series(best1)[pd.Series(best1).isin(best2)]\n",
    "\n",
    "def get_retframe(bestnr, art=None):\n",
    "    if art:\n",
    "        return train[(train[\"basketID\"].isin(bestnr)) & (train[\"itemID\"]==art)]\n",
    "    else:\n",
    "        return train[train[\"basketID\"].isin(bestnr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d77071a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all transactions with duplicated item in basket\n",
    "no_dub = dff[dff[[\"basketID\", \"itemID\"]].duplicated(keep=False)]\n",
    "data_meta_grouped = dff.groupby([\"basketID\", \"itemID\"])[\"returnShipment\"].mean()\n",
    "data_meta_index = no_dub.drop_duplicates(keep=False).groupby([\"basketID\", \"itemID\"]).size().index\n",
    "repair_data = data_meta_grouped.loc[data_meta_index].reset_index()\n",
    "repair_data.loc[:, meta_col] = meta_data.loc[repair_data[meta_col]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "947aef8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basketID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>returnShipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100012012-04-17</td>\n",
       "      <td>909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002012-04-02</td>\n",
       "      <td>278</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002012-09-26</td>\n",
       "      <td>1470</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002012-09-26</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100112012-04-18</td>\n",
       "      <td>86</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>99942012-04-17</td>\n",
       "      <td>217</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>99982012-04-17</td>\n",
       "      <td>32</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19924</th>\n",
       "      <td>99982012-09-17</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19925</th>\n",
       "      <td>99992012-09-14</td>\n",
       "      <td>1745</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19926</th>\n",
       "      <td>99992012-11-18</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19927 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              basketID  itemID  returnShipment\n",
       "0      100012012-04-17     909        0.500000\n",
       "1       10002012-04-02     278        0.500000\n",
       "2       10002012-09-26    1470        0.500000\n",
       "3       10002012-09-26    1704        0.500000\n",
       "4      100112012-04-18      86        0.500000\n",
       "...                ...     ...             ...\n",
       "19922   99942012-04-17     217        0.666667\n",
       "19923   99982012-04-17      32        0.500000\n",
       "19924   99982012-09-17    1488        0.500000\n",
       "19925   99992012-09-14    1745        0.500000\n",
       "19926   99992012-11-18    1401        0.666667\n",
       "\n",
       "[19927 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8550105",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in repair_data.itertuples():\n",
    "                n.loc[x[2], x[1]] -= x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "37b926c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basketID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>returnShipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[229, 244, 909]</td>\n",
       "      <td>909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[71, 278, 287]</td>\n",
       "      <td>278</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[707]</td>\n",
       "      <td>707</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1415, 1470, 1532, 1615, 1704]</td>\n",
       "      <td>1470</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1415, 1470, 1532, 1615, 1704]</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23619</th>\n",
       "      <td>[296]</td>\n",
       "      <td>296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23620</th>\n",
       "      <td>[32, 59, 87, 125, 128, 263, 331, 448, 532]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23621</th>\n",
       "      <td>[233, 804, 1470, 1488, 1518, 1620, 1636, 1656,...</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23622</th>\n",
       "      <td>[1526, 1745, 1760]</td>\n",
       "      <td>1745</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>[1401, 1503]</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23624 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                basketID  itemID  \\\n",
       "0                                        [229, 244, 909]     909   \n",
       "1                                         [71, 278, 287]     278   \n",
       "2                                                  [707]     707   \n",
       "3                         [1415, 1470, 1532, 1615, 1704]    1470   \n",
       "4                         [1415, 1470, 1532, 1615, 1704]    1704   \n",
       "...                                                  ...     ...   \n",
       "23619                                              [296]     296   \n",
       "23620         [32, 59, 87, 125, 128, 263, 331, 448, 532]      32   \n",
       "23621  [233, 804, 1470, 1488, 1518, 1620, 1636, 1656,...    1488   \n",
       "23622                                 [1526, 1745, 1760]    1745   \n",
       "23623                                       [1401, 1503]    1401   \n",
       "\n",
       "       returnShipment  \n",
       "0            0.500000  \n",
       "1            0.500000  \n",
       "2            0.500000  \n",
       "3            0.500000  \n",
       "4            0.500000  \n",
       "...               ...  \n",
       "23619        0.500000  \n",
       "23620        0.500000  \n",
       "23621        0.500000  \n",
       "23622        0.500000  \n",
       "23623        0.666667  \n",
       "\n",
       "[23624 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_data.apply(lambda x: n.loc[x[\"itemID\"], x[\"basketID\"]] -= x[\"returnShipment\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f4782f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4107ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_dub.drop_duplicates(keep=False).groupby([\"basketID\", \"itemID\"]).size().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3f1e3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff[dff[[\"basketID\", \"itemID\", \"returnShipment\"]].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afcadcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderDate</th>\n",
       "      <th>deliveryDate</th>\n",
       "      <th>itemID</th>\n",
       "      <th>size</th>\n",
       "      <th>color</th>\n",
       "      <th>manufacturerID</th>\n",
       "      <th>price</th>\n",
       "      <th>customerID</th>\n",
       "      <th>salutation</th>\n",
       "      <th>dateOfBirth</th>\n",
       "      <th>state</th>\n",
       "      <th>creationDate</th>\n",
       "      <th>returnShipment</th>\n",
       "      <th>val_set</th>\n",
       "      <th>basketID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29613</th>\n",
       "      <td>2012-04-23</td>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "      <td>119.9</td>\n",
       "      <td>12237</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baden-Wuerttemberg</td>\n",
       "      <td>2012-04-23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122372012-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79932</th>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>2012-06-06</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>ocher</td>\n",
       "      <td>1</td>\n",
       "      <td>119.9</td>\n",
       "      <td>26119</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1963-04-12</td>\n",
       "      <td>North Rhine-Westphalia</td>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>261192012-06-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87747</th>\n",
       "      <td>2012-06-11</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>mocca</td>\n",
       "      <td>1</td>\n",
       "      <td>119.9</td>\n",
       "      <td>28085</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1963-04-13</td>\n",
       "      <td>Bavaria</td>\n",
       "      <td>2011-02-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280852012-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        orderDate deliveryDate  itemID size  color  manufacturerID  price  \\\n",
       "29613  2012-04-23   2012-04-27       1   40   blue               1  119.9   \n",
       "79932  2012-06-03   2012-06-06       1   37  ocher               1  119.9   \n",
       "87747  2012-06-11   2012-06-14       1   39  mocca               1  119.9   \n",
       "\n",
       "       customerID salutation dateOfBirth                   state creationDate  \\\n",
       "29613       12237        Mrs         NaN      Baden-Wuerttemberg   2012-04-23   \n",
       "79932       26119        Mrs  1963-04-12  North Rhine-Westphalia   2012-01-18   \n",
       "87747       28085        Mrs  1963-04-13                 Bavaria   2011-02-16   \n",
       "\n",
       "       returnShipment  val_set         basketID  \n",
       "29613               1        0  122372012-04-23  \n",
       "79932               0        0  261192012-06-03  \n",
       "87747               0        0  280852012-06-11  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_retframe(check_joint_arts(1, 5), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: beide arts in einem Warenkorb, ein return einer so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7fa58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"train_f\"\n",
    "\n",
    "arts_posterior[name] = values_i\n",
    "\n",
    "    # reset index to arts (only articles that have been in orders)\n",
    "Ni_prior = Ni_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "alpha_i_prior = alpha_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "beta_i_prior = beta_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "values_i = values_i.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "ni = ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "Ni = Ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "\n",
    "# second stage: data<>meta-group combinations\n",
    "nij = ns[i].copy()\n",
    "Nij = Ns[i].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c384401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12068/2361313778.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n",
      "/tmp/ipykernel_12068/2361313778.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects)\n"
     ]
    }
   ],
   "source": [
    "prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects) \n",
    "inds_zero = np.where(np.isnan(prior_means))\n",
    "prior_means[inds_zero] = np.take(values_i, inds_zero[0]) # replace nas with article-wise priors\n",
    "# if N=1 the regularization can cause division by 0 -> replace with mean\n",
    "inds_inf = np.where(np.isinf(prior_means))\n",
    "prior_means[inds_inf] = np.take(values_i, inds[inds_inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9893b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "infies = pd.DataFrame(prior_means).apply(lambda x: (x>1).sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a748053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[enc_train[enc_train==1].index, \"basketID\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d6ddcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.arts_posterior[\"train_t\"].loc[train.loc[enc_train[enc_train==1].index, \"itemID\"].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25594bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12068/1397208451.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  alpha_ij_prior = alpha_i_prior+prior_means*Nij_prior\n",
      "/tmp/ipykernel_12068/1397208451.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  beta_ij_prior = beta_i_prior+(1-prior_means)*Nij_prior\n"
     ]
    }
   ],
   "source": [
    "Nij_prior = np.maximum(N_min-Nij, 0) \n",
    "Nij_prior = np.maximum(Nij_prior - Ni_prior,0) # Nij_prior >= Ni_prior\n",
    "\n",
    "beta_ij_prior = beta_i_prior+(1-prior_means)*Nij_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c9c8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12068/363528080.py:1: RuntimeWarning: invalid value encountered in multiply\n",
      "  alpha_ij_prior = alpha_i_prior + (prior_means*Nij_prior)\n"
     ]
    }
   ],
   "source": [
    "alpha_ij_prior = alpha_i_prior + (prior_means*Nij_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d94be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayLOO2d():\n",
    "    def __init__(self, N_min = 20, variants=False):\n",
    "        self.variants = variants\n",
    "        self.N_min = N_min\n",
    "    def fit(self, data, targets, meta_group):\n",
    "        N_min = self.N_min\n",
    "        # get colnames (using pandas .name attribute -> needed atm)\n",
    "        data_col = data.name\n",
    "        target_col = targets.name\n",
    "        meta_col = meta_group.name\n",
    "\n",
    "        # generate data used for aricle-wise stage\n",
    "        prior_mean = targets.mean() # mean of dataset\n",
    "        self.prior_mean = prior_mean\n",
    "\n",
    "        df = pd.merge(data, targets, left_index = True, right_index = True)\n",
    "\n",
    "        # first stage: equal to (leave one out) beta encoder\n",
    "        stats = df.groupby(data_col)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]\n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats = stats.reset_index(level=0).set_index(data_col)\n",
    "        # generate arts_lookup for single-arts\n",
    "        stats_f = stats.copy()\n",
    "        stats_f.loc[:, \"N\"] = stats_f[\"N\"] - 1\n",
    "        # stats_f.loc[stats_f[\"N\"]>1, \"N\"] = stats_f[stats_f[\"N\"]>1][\"N\"] - 1\n",
    "        stats_t = stats_f.copy()\n",
    "        stats_t.loc[stats_t[\"n\"]>0, \"n\"] = stats_f[\"n\"] - 1\n",
    "\n",
    "        # first stage: article-meta stage for article-basket data\n",
    "        dff = pd.merge(meta_group, df, left_index = True, right_index = True)\n",
    "\n",
    "        #\n",
    "        meta_data = dff.groupby(meta_col)[data_col].apply(np.array)\n",
    "        meta_data = meta_data.apply(np.unique) # inserted\n",
    "\n",
    "        meta_lens = meta_data.apply(lambda x: len(x))\n",
    "        meta_multi = meta_lens[meta_lens>1].index.to_series()\n",
    "\n",
    "        meta_arts = np.sort(dff[dff[meta_col].isin(meta_multi)][data_col].unique())\n",
    "        self.meta_arts = pd.Series(meta_arts, name=data_col)\n",
    "\n",
    "        dff = dff[dff[meta_col].isin(meta_multi)] # change )&  (dff[data_col].isin(meta_arts))\n",
    "        dff_rets = dff[dff[target_col]==1]\n",
    "        \n",
    "        data_meta = dff.groupby(data_col)[meta_col].apply(np.array)\n",
    "        data_meta = data_meta.apply(lambda x: np.unique(x)) # unique: dont take multi-arts in same basket into account\n",
    "        data_ret_meta = dff_rets.groupby(data_col)[meta_col].apply(np.array)\n",
    "        data_ret_meta = data_ret_meta.apply(lambda x: np.unique(x))\n",
    "\n",
    "        joint_datapoints_N = data_meta.apply(lambda x: np.concatenate(meta_data.loc[x].to_numpy())) \n",
    "        joint_datapoints_n = data_ret_meta.apply(lambda x: np.concatenate(meta_data.loc[x].to_numpy()))\n",
    "\n",
    "        counted_N = joint_datapoints_N.apply(lambda x: np.unique(x, return_counts=True))\n",
    "        counted_n = joint_datapoints_n.apply(lambda x: np.unique(x, return_counts=True))\n",
    "\n",
    "        N = counted_N.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "        n = counted_n.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "        N = N.loc[meta_arts, meta_arts]\n",
    "        n = n.reindex(meta_arts, axis=1).reindex(meta_arts)\n",
    "\n",
    "        if self.variants == True:\n",
    "            # problem -> get \n",
    "            no_dub = dff[dff[[\"basketID\", \"itemID\"]].duplicated(keep=False)]\n",
    "            data_meta_grouped = dff.groupby([\"basketID\", \"itemID\"])[\"returnShipment\"].mean()\n",
    "            data_meta_index = no_dub.drop_duplicates(keep=False).groupby([\"basketID\", \"itemID\"]).size().index\n",
    "            repair_data = data_meta_grouped.loc[data_meta_index].reset_index()\n",
    "            repair_data.loc[:, meta_col] = meta_data.loc[repair_data[meta_col]].to_numpy()\n",
    "            for x in repair_data.itertuples():\n",
    "                n.loc[x[2], x[1]] -= x[3]\n",
    "            for art in meta_arts:\n",
    "                n.loc[art, art] = prior_mean\n",
    "                N.loc[art,art] = 1\n",
    "            grouped = dff.groupby(data_col)[meta_col].value_counts()\n",
    "            grouped_N = grouped[grouped>1]\n",
    "            \n",
    "            # variants -> stattdessen count?\n",
    "            variants_N = grouped_N.groupby(data_col).sum()\n",
    "            # drops = alle arts die \n",
    "            drops = grouped[grouped==1]\n",
    "            grouped_ret = dff_rets.groupby(data_col)[meta_col].value_counts()\n",
    "            drop_inc = drops.index.get_level_values(0).unique().to_series()\n",
    "            ret_inc = grouped_ret.index.get_level_values(0).unique().to_series()\n",
    "            # drop_inc: only arts that got returned (grouped_ret)\n",
    "            drop_inc = drop_inc[drop_inc.isin(ret_inc)]\n",
    "            for inc in drop_inc:\n",
    "                # if grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore'):\n",
    "                n.loc[inc,inc] = grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore').sum()        \n",
    "            for item in variants_N.index:\n",
    "                N.loc[item, item] = variants_N.loc[item]\n",
    "\n",
    "            \"\"\"multi_counts = data_meta.apply(lambda x: len(x))\n",
    "            # auch bei returns\n",
    "            multi_ret_counts = data_ret_meta.apply(lambda x: len(x))\n",
    "            # abziehen von df\n",
    "            for item in multi_counts.index:\n",
    "                N.loc[item, item] = N.loc[item, item] - multi_counts.loc[item]\n",
    "            for item in multi_ret_counts.index:\n",
    "                n.loc[item, item] = n.loc[item, item] - multi_ret_counts.loc[item]\"\"\"\n",
    "        #else:\n",
    "        #    for item in meta_arts:\n",
    "        #        N.loc[item, item] = np.nan\n",
    "        #        n.loc[item, item] = np.nan\n",
    "\n",
    "        N_train = np.nan_to_num((N - 1).to_numpy(dtype=float))# .astype(int)\n",
    "        N = np.nan_to_num(N.to_numpy(dtype=float))# .astype(int)\n",
    "        n_train_t = np.nan_to_num((n-1).to_numpy(dtype=float))# .astype(int)\n",
    "        n = np.nan_to_num(n.to_numpy(dtype=float))# .astype(int)\n",
    "\n",
    "        # generate lift and confidence for sorting\n",
    "        sorting = {}\n",
    "        n_transactions = data.shape[0]\n",
    "        rel_freq_arts = (stats.loc[meta_arts, \"N\"]/n_transactions).to_numpy()\n",
    "        lift_frame = (N/n_transactions)/ (np.atleast_2d(rel_freq_arts).T * rel_freq_arts)\n",
    "        lift_frame[np.where(lift_frame==0)] = 1\n",
    "        sorting[\"lift\"] = np.argsort(lift_frame, axis=1)\n",
    "        sorting[\"conf\"] = np.argsort(N, axis=1)\n",
    "        self.sorting = sorting\n",
    "\n",
    "        frame_name = [\"train_f\", \"train_t\", \"test\"]\n",
    "\n",
    "        arts_posterior = {}\n",
    "        stats_frames = [stats_f, stats_t, stats]\n",
    "\n",
    "        arts_ij_posterior = {}\n",
    "        ns = [n,n_train_t, n]\n",
    "        Ns = [N_train, N_train, N]\n",
    "        # multi_statsframes = [multi_f_stats, multi_t_stats, multi_stats]\n",
    "        for i, name in enumerate(frame_name):\n",
    "            statsframe = stats_frames[i].copy()\n",
    "            ni = statsframe[\"n\"]\n",
    "            Ni = statsframe[\"N\"]\n",
    "            # Stufe 1: data-column based -> create stuff based on index of arts; other stuff seperately\n",
    "                # if Ni (Anzahl Artikel i) > Ni_prior: prior = 0; else N_min-AnzahlArts\n",
    "            Ni_prior = np.maximum(N_min-Ni, 0) # 1D-array\n",
    "\n",
    "            # bayes regularization for articles i: use mean_prior from dataset (no leave-one-out)\n",
    "            alpha_i_prior = prior_mean*Ni_prior\n",
    "            beta_i_prior = (1-prior_mean)*Ni_prior\n",
    "\n",
    "            alpha_i = alpha_i_prior + ni\n",
    "            beta_i = beta_i_prior + (Ni-ni)\n",
    "            values_i = alpha_i/(alpha_i+beta_i)\n",
    "            # option -> if no variants: generate addidional alpha etc. from multi-art orders\n",
    "\n",
    "            arts_posterior[name] = values_i\n",
    "\n",
    "                # reset index to arts (only articles that have been in orders)\n",
    "            Ni_prior = Ni_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            alpha_i_prior = alpha_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            beta_i_prior = beta_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            values_i = values_i.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            ni = ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            Ni = Ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "\n",
    "            # second stage: data<>meta-group combinations\n",
    "            nij = ns[i].copy()\n",
    "            Nij = Ns[i].copy()\n",
    "\n",
    "            # bayes regularization for article combinations ij\n",
    "                # leaves out evidence from existing combination ij (e.g. Ni-N)\n",
    "            nij_reg = ni-nij # number of positive observations *not* including combination nij\n",
    "            Nij_reg = Ni-Nij # number of total observations *not* including combination nij\n",
    "\n",
    "            prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects) \n",
    "            inds_zero = np.where(np.isnan(prior_means))\n",
    "            prior_means[inds_zero] = np.take(values_i, inds_zero[0]) # replace nas with article-wise priors\n",
    "            # if N=1 the regularization can cause division by 0 -> replace with mean\n",
    "            inds_inf = np.where(np.isinf(prior_means))\n",
    "            prior_means[inds_inf] = np.take(values_i, inds_inf[0])\n",
    "\n",
    "            Nij_prior = np.maximum(N_min-Nij, 0) \n",
    "            Nij_prior = np.maximum(Nij_prior - Ni_prior,0) # Nij_prior >= Ni_prior\n",
    "\n",
    "            alpha_ij_prior = alpha_i_prior+prior_means*Nij_prior\n",
    "            beta_ij_prior = beta_i_prior+(1-prior_means)*Nij_prior\n",
    "\n",
    "            alpha_ij = alpha_ij_prior + nij\n",
    "            beta_ij = beta_ij_prior + (Nij-nij)\n",
    "\n",
    "            values_ij = alpha_ij/(alpha_ij+beta_ij)\n",
    "\n",
    "            arts_ij_posterior[name] = values_ij\n",
    "        self.arts_posterior = arts_posterior\n",
    "        self.arts_ij_posterior = arts_ij_posterior\n",
    "        return self # [arts_posterior, arts_ij_posterior, meta_arts, sorting, prior_mean]\n",
    "        \n",
    "    def transform(self, data, meta_group, targets=None, weighted=True, basket_max=5, sorting_type=\"lift\"):\n",
    "        \"\"\"meta_arts as indexer, since arts_ij_posterior is an array\"\"\"            \n",
    "        arts_posterior = self.arts_posterior\n",
    "        arts_ij_posterior = self.arts_ij_posterior\n",
    "        prior_mean = self.prior_mean\n",
    "        meta_arts = self.meta_arts\n",
    "        sorting = self.sorting[sorting_type]\n",
    "        \n",
    "        data_col = data.name\n",
    "        meta_col = meta_group.name\n",
    "        \n",
    "        # working with int for meta-group\n",
    "        if meta_group.dtype != int:\n",
    "            new_ind = pd.Series(index=meta_group.unique(), data = range(meta_group.nunique()))\n",
    "            meta_group = pd.Series(new_ind.loc[meta_group].to_numpy(), index = meta_group.index, name=meta_col)\n",
    "        df = pd.merge(data, meta_group, left_index = True, right_index = True)\n",
    "\n",
    "        # drop new datapoints and set them to prior mean\n",
    "            # will not use them for calculating basket_mean as well\n",
    "        known_data = arts_posterior[\"train_f\"].index.to_series()\n",
    "        new_data = df[~df[data_col].isin(known_data)].index\n",
    "        df.drop(new_data, inplace=True)\n",
    "        new_data = pd.Series(prior_mean, index = new_data)\n",
    "\n",
    "        # replace meta-category to new dummy for datapoints not in meta_arts\n",
    "        new_meta_inc = df[~df[data_col].isin(meta_arts)].index\n",
    "        new_meta_imp = max(df[meta_col])+1\n",
    "        df.loc[new_meta_inc, meta_col] = range(new_meta_imp, new_meta_imp+len(new_meta_inc))\n",
    "\n",
    "        # drop datapoints with no joint meta-categoty (including dummmy)\n",
    "        arts_inc = df[meta_col].drop_duplicates(keep=False).index\n",
    "        df_arts = df.loc[arts_inc]\n",
    "        arts_data = pd.Series(index=arts_inc, dtype=float)\n",
    "        df.drop(arts_inc, inplace=True)\n",
    "\n",
    "        # set categories in data to integer (matching arts_ij_posterior index)\n",
    "        meta_arts_inv = meta_arts.reset_index().set_index(data_col)\n",
    "        df[data_col] = meta_arts_inv.loc[df[data_col]][\"index\"].to_list() # \n",
    "        setted = df.groupby(meta_col)[data_col].agg(\"unique\")\n",
    "        if self.variants == True:\n",
    "            singles = df[[data_col, meta_col]].drop_duplicates(keep=False)\n",
    "            singles = singles.apply(lambda x:\n",
    "                                        setted.loc[x[meta_col]]\n",
    "                                        [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "            doubles = df[df[[data_col, meta_col]].duplicated(keep=False)].apply(lambda x:\n",
    "                                        setted.loc[x[meta_col]], axis=1)\n",
    "            data_basket = pd.concat([singles, doubles]).sort_index()\n",
    "        else:\n",
    "            data_basket = df.apply(lambda x:\n",
    "                                setted.loc[x[meta_col]]\n",
    "                                [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "        df[\"basket\"] = data_basket\n",
    "        # basket_arts = basket[~basket.str.len().eq(0)].index\n",
    "        basket_data = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "        if type(targets) == pd.core.series.Series:\n",
    "            target_col = targets.name\n",
    "            df_arts = pd.merge(df_arts, targets, left_index = True, right_index = True)\n",
    "            df = pd.merge(df, targets, left_index = True, right_index = True)\n",
    "\n",
    "            # entries in meta-group\n",
    "            # df_arts = dff.loc[arts_inc]\n",
    "            # df_basket_arts = dff.loc[basket_arts]\n",
    "            for i, trainset in enumerate(['train_f', 'train_t']):\n",
    "                arts_get = df_arts[df_arts[target_col]==i]\n",
    "                arts_lookup = arts_posterior[trainset]\n",
    "                arts_data.loc[arts_get.index] = arts_lookup.loc[arts_get[data_col]].to_list()\n",
    "\n",
    "                basket_get = df[df[target_col]==i]\n",
    "                basket_lookup = arts_ij_posterior[trainset]\n",
    "                ij_baskets = basket_get.apply(lambda x:\n",
    "                                              basket_lookup[x[data_col],\n",
    "                                                            x[\"basket\"][np.argsort(sorting[x[data_col],x[\"basket\"]])][-basket_max:]],\n",
    "                                              axis=1)   # sorts basket according to sorting and picks the articles accordingly\n",
    "                if weighted == True:\n",
    "                    weights = ij_baskets.apply(lambda x: range(1, len(x)+1))\n",
    "                    weigthed = (weights * ij_baskets)\n",
    "                    avg = weigthed.apply(lambda x: sum(x)\n",
    "                                        ) / weights.apply(lambda x: sum(x))\n",
    "                else:\n",
    "                    avg = ij_baskets.apply(lambda x: x.mean())\n",
    "                basket_data.loc[basket_get.index] = avg.to_list()\n",
    "        else:\n",
    "            arts_lookup = arts_posterior['test']\n",
    "            arts_data.loc[arts_inc] = arts_lookup.loc[df_arts[data_col]].to_list()\n",
    "\n",
    "            basket_lookup = arts_ij_posterior['test']\n",
    "            ij_baskets = df.apply(lambda x: \n",
    "                                  basket_lookup[x[data_col],\n",
    "                                                x[\"basket\"][np.argsort(sorting[x[data_col],x[\"basket\"]])][-basket_max:]],\n",
    "                                  axis=1)\n",
    "            if weighted == True:\n",
    "                weights = ij_baskets.apply(lambda x: range(1, len(x)+1))\n",
    "                weigthed = (weights * ij_baskets)\n",
    "                avg = weigthed.apply(lambda x: sum(x)\n",
    "                                    ) / weights.apply(lambda x: sum(x))\n",
    "                # print(avg.head())\n",
    "            else:\n",
    "                avg = ij_baskets.apply(lambda x: x.mean())\n",
    "                # print(avg.head())\n",
    "            basket_data.loc[avg.index] = avg.to_list()\n",
    "        values = pd.concat([new_data, arts_data, basket_data])\n",
    "        self.values_distibution = {\n",
    "            \"new\": new_data.shape[0],\n",
    "            \"arts\": arts_data.shape[0],\n",
    "            \"basket\": basket_data.shape[0]}\n",
    "        \n",
    "        return values.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2696c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dmc_2014/dmc14/lib/python3.8/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "\n",
    "n_min = 20\n",
    "mEnc = MEstimateEncoder(m=n_min, cols=data_col)\n",
    "# mEnc_rand = MEstimateEncoder(m=20, random_state=42, randomized=True)\n",
    "enc_train = mEnc.fit_transform(train[data_col], train[target_col]).iloc[:, 0]\n",
    "enc_test = mEnc.transform(test[data_col]).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2c328b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5273445080736244\\n\\n0.46653558503547077\\n\\n0.553600347994004\\n\\n0.4832551883144125'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values for BayLOO2d encoder\n",
    "\"\"\"0.5273445080736244\n",
    "\n",
    "0.46653558503547077\n",
    "\n",
    "0.553600347994004\n",
    "\n",
    "0.4832551883144125\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ea5a8f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5198234935321024 0.448691804286501 0.546005061750093 0.48251791860307236\n"
     ]
    }
   ],
   "source": [
    "# mEnc.fit_transform(train[data_col], train[target_col])\n",
    "print(enc_train.loc[train[train[target_col]==1].index].mean(),\n",
    "    enc_train.loc[train[train[target_col]==0].index].mean(),\n",
    "    enc_test.loc[test[test[target_col]==1].index].mean(),\n",
    "    enc_test.loc[test[test[target_col]==0].index].mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde1912",
   "metadata": {},
   "source": [
    "## Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "978b9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[\"itemID\"]\n",
    "meta_group = train[\"basketID\"]\n",
    "targets=train[\"returnShipment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1f096e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([    71,   1942,   4045,   4365,   5280,   5541,   6498,   6807,\n",
      "              6926,   8735,\n",
      "            ...\n",
      "            380667, 388763, 397097, 400793, 404935, 416352, 429319, 437751,\n",
      "            437852, 454815],\n",
      "           dtype='int64', length=122) 138731\n"
     ]
    }
   ],
   "source": [
    "weighted=True\n",
    "basket_max=5\n",
    "sorting_type=\"lift\"\n",
    "\n",
    "arts_posterior = enc.arts_posterior\n",
    "arts_ij_posterior = enc.arts_ij_posterior\n",
    "prior_mean = enc.prior_mean\n",
    "meta_arts = enc.meta_arts\n",
    "sorting = enc.sorting[sorting_type]\n",
    "\n",
    "data_col = data.name\n",
    "meta_col = meta_group.name\n",
    "\n",
    "# working with int for meta-group\n",
    "if meta_group.dtype != int:\n",
    "    new_ind = pd.Series(index=meta_group.unique(), data = range(meta_group.nunique()))\n",
    "    meta_group = pd.Series(new_ind.loc[meta_group].to_numpy(), index = meta_group.index, name=meta_col)\n",
    "df = pd.merge(data, meta_group, left_index = True, right_index = True)\n",
    "\n",
    "# drop new datapoints and set them to prior mean\n",
    "    # will not use them for calculating basket_mean as well\n",
    "known_data = arts_posterior[\"train_f\"].index.to_series()\n",
    "new_data = df[~df[data_col].isin(known_data)].index\n",
    "df.drop(new_data, inplace=True)\n",
    "new_data = pd.Series(prior_mean, index = new_data)\n",
    "\n",
    "# replace meta-category to new dummy for datapoints not in meta_arts\n",
    "new_meta_inc = df[~df[data_col].isin(meta_arts)].index\n",
    "new_meta_imp = max(df[meta_col])+1\n",
    "print(new_meta_inc, new_meta_imp)\n",
    "df.loc[new_meta_inc, meta_col] = range(new_meta_imp, new_meta_imp+len(new_meta_inc))\n",
    "\n",
    "# drop datapoints with no joint meta-categoty (including dummmy)\n",
    "arts_inc = df[meta_col].drop_duplicates(keep=False).index\n",
    "df_arts = df.loc[arts_inc]\n",
    "arts_data = pd.Series(index=arts_inc, dtype=float)\n",
    "df.drop(arts_inc, inplace=True)\n",
    "\n",
    "# set categories in data to integer (matching arts_ij_posterior index)\n",
    "# meta_arts_inv = meta_arts.reset_index().set_index(data_col)\n",
    "df[data_col] = meta_arts.loc[df[data_col]].to_list() # [\"index\"]\n",
    "setted = df.groupby(meta_col)[data_col].agg(\"unique\")\n",
    "df[\"basket\"] = df.apply(lambda x:\n",
    "                        setted.loc[x[meta_col]]\n",
    "                        [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "# basket_arts = basket[~basket.str.len().eq(0)].index\n",
    "basket_data = pd.Series(index=df.index, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = df[[data_col, meta_col]].drop_duplicates(keep=False)\n",
    "singles = singles.apply(lambda x:\n",
    "                            setted.loc[x[meta_col]]\n",
    "                            [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "doubles = df[df[[data_col, meta_col]].duplicated(keep=False)].apply(lambda x:\n",
    "                            setted.loc[x[meta_col]], axis=1)\n",
    "pd.concat([singles, doubles]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "80db1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = targets.name\n",
    "df_arts = pd.merge(df_arts, targets, left_index = True, right_index = True)\n",
    "df = pd.merge(df, targets, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4994396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "trainset = \"train_f\"\n",
    "arts_get = df_arts[df_arts[target_col]==i]\n",
    "arts_lookup = arts_posterior[trainset]\n",
    "arts_data.loc[arts_get.index] = arts_lookup.loc[arts_get[data_col]].to_list()\n",
    "\n",
    "basket_get = df[df[target_col]==i]\n",
    "basket_lookup = arts_ij_posterior[trainset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe7798",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    # generate lookups\n",
    "        # baskets: df with all data_col <> meta_col combinations\n",
    "    baskets = {}\n",
    "    for bestnr in dff[dff[meta_col].duplicated(keep=False)][meta_col].unique():\n",
    "        baskets[bestnr] = dff[dff[meta_col]==bestnr][data_col]\n",
    "    baskets = pd.concat(baskets)\n",
    "    baskets = baskets.reset_index().rename({\"level_0\": meta_col}, axis=1)\n",
    "    baskets = baskets.set_index(\"level_1\")\n",
    "    baskets[target_col] = targets\n",
    "\n",
    "    # arts -> used for indexing: all articles, which are in multi-article orders\n",
    "    meta_arts = baskets[data_col].drop_duplicates().reset_index(drop=True)\n",
    "    self.meta_arts = meta_arts\n",
    "\n",
    "        # data(_true)_baskets -> lookup for all (true) datapoints<>meta_group combinations\n",
    "    data_baskets = {}\n",
    "    data_true_baskets = {}\n",
    "    for article in meta_arts: # kept article (instead of datapoint) for better readability\n",
    "        art_df = dff[dff[data_col]==article]\n",
    "        data_baskets[article] = art_df[meta_col].unique()\n",
    "        data_true_baskets[article] = art_df[art_df[target_col]==1][meta_col].unique()\n",
    "\n",
    "    # N = total numbers of articles sharing meta-group (shopping basket) and n for target=1\n",
    "        # optionA: using dataframes indexing -> optional numpy(?)\n",
    "    N = meta_arts.apply(lambda x: baskets[(baskets[meta_col].isin(\n",
    "        data_baskets[x]))& (baskets[data_col]!=x)][data_col].value_counts()).set_index(meta_arts)\n",
    "    N = N[N.index] # set symmetrical\n",
    "    # for multi-arts in basket -> (1) adapt N for own article (duplicate baskets)\n",
    "    n = meta_arts.apply(lambda x: baskets[(baskets[meta_col].isin(\n",
    "        data_true_baskets[x]))& (baskets[data_col]!=x)][data_col].value_counts()).set_index(meta_arts)  \n",
    "# raus\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input data\n",
    "N_min = 20\n",
    "data = train[\"ARTIKELNR\"]\n",
    "targets = train[\"RET_SUB\"]\n",
    "meta_group = train[\"BEST_NR\"]\n",
    "df = pd.merge(data, targets, left_index = True, right_index = True)\n",
    "dff = pd.merge(meta_group, df, left_index = True, right_index = True)\n",
    "data_col = data.name\n",
    "target_col = targets.name\n",
    "meta_col = meta_group.name\n",
    "prior_mean = targets.mean()\n",
    "\n",
    "## generate df with all basket combinations\n",
    "baskets = {}\n",
    "for bestnr in dff[dff[\"BEST_NR\"].duplicated(keep=False)][\"BEST_NR\"].unique():\n",
    "    baskets[bestnr] = dff[dff[\"BEST_NR\"]==bestnr][\"ARTIKELNR\"]\n",
    "baskets = pd.concat(baskets)\n",
    "baskets = baskets.reset_index().rename({\"level_0\": \"BEST_NR\"}, axis=1)\n",
    "baskets = baskets.set_index(\"level_1\")\n",
    "baskets[\"RET_SUB\"] = targets\n",
    "# optional -> faster\n",
    "    # dff.groupby(\"BEST_NR\")[\"ARTIKELNR\"].agg(['unique'])\n",
    "        # also later applicable\n",
    "meta_arts = baskets[\"ARTIKELNR\"].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "stats = df.groupby(data.name)\n",
    "stats = stats.agg(['sum', 'count'])[targets.name]\n",
    "stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "stats.reset_index(level=0, inplace=True)\n",
    "\n",
    "stats_f = stats.copy()\n",
    "stats_f.loc[stats_f[\"N\"]>1, \"N\"] = stats_f[stats_f[\"N\"]>1][\"N\"] - 1\n",
    "stats_t = stats_f.copy()\n",
    "stats_t.loc[stats_t[\"n\"]>0, \"n\"] = stats_f[\"n\"] - 1\n",
    "\n",
    "# stats object for multi-arts = arts that share meta-group\n",
    "    # set (sort) to general index (arts)\n",
    "multi_stats = stats.set_index(data_col).loc[arts]\n",
    "multi_f_stats = stats_f.set_index(data_col).loc[arts]\n",
    "multi_t_stats = stats_t.set_index(data_col).loc[arts]\n",
    "\n",
    "data_baskets = {}\n",
    "data_true_baskets = {}\n",
    "# arts = dff[dff[\"BEST_NR\"].duplicated(keep=False)][\"ARTIKELNR\"].unique()\n",
    "for article in arts:\n",
    "    art_df = dff[dff[\"ARTIKELNR\"]==article]\n",
    "    data_baskets[article] = art_df[\"BEST_NR\"].unique()\n",
    "    data_true_baskets[article] = art_df[art_df[\"RET_SUB\"]==1][\"BEST_NR\"].unique()\n",
    "# maybe create duplicate baskets here\n",
    "\n",
    "# optionA: using dataframes indexing -> optional numpy(?)\n",
    "N = meta_arts.apply(lambda x: baskets[(baskets[\"BEST_NR\"].isin(\n",
    "    data_baskets[x]))& (baskets[\"ARTIKELNR\"]!=x)][\"ARTIKELNR\"].value_counts()).set_index(meta_arts)\n",
    "\n",
    "N = N[N.index] # set symmetrical\n",
    "\n",
    "# for multi-arts in basket -> (1) adapt N for own article (duplicate baskets)\n",
    "n = meta_arts.apply(lambda x: baskets[(baskets[\"BEST_NR\"].isin(\n",
    "    data_true_baskets[x]))& (baskets[\"ARTIKELNR\"]!=x)][\"ARTIKELNR\"].value_counts()).set_index(meta_arts)\n",
    "# misses columns in n if article was not in basket with return\n",
    "N_cols = N.columns.to_series()\n",
    "non_n_df = pd.DataFrame(columns= N_cols[~N_cols.isin(n.columns)], index = N.index)\n",
    "n = pd.merge(n, non_n_df, left_index=True, right_index = True)\n",
    "# n is not symmetrical: it depends on which articles get returned together\n",
    "n = n[N.columns]\n",
    "\n",
    "N_train = np.nan_to_num((N - 1).to_numpy(dtype=float)).astype(int)\n",
    "N = np.nan_to_num(N.to_numpy(dtype=float)).astype(int)\n",
    "\n",
    "n_train_t = np.nan_to_num((n-1).to_numpy(dtype=float)).astype(int)\n",
    "n = np.nan_to_num(n.to_numpy(dtype=float)).astype(int)\n",
    "\n",
    "ns = [n_train_t, n, n]\n",
    "Ns = [N_train, N_train, N]\n",
    "# np.nan_to_num(N.to_numpy(dtype=float)).astype(int)\n",
    "\n",
    "lookup_lift_arts = catalog.load(\"lookup_lift_arts\")\n",
    "lookup_basket_occ = catalog.load(\"lookup_basket_occ\")\n",
    "train[train[\"BEST_NR\"].duplicated(keep=False)][\"BEST_NR\"].nunique()\n",
    "\n",
    "# check for occs\n",
    "i = 0\n",
    "arts=[]\n",
    "for art in lookup_basket_occ[0].keys():\n",
    "    arts += [art]\n",
    "    sliced = lookup_basket_occ[0][art]\n",
    "    coms = comb.loc[sliced.index, art] - sliced\n",
    "    i += coms.sum()\n",
    "    print(coms.dropna().shape[0])\n",
    "    \n",
    "data_col = data.name\n",
    "target_col = targets.name\n",
    "meta_col = meta_group.name\n",
    "df = pd.merge(data, meta_group, left_index = True, right_index = True)\n",
    "\n",
    "# drop new datapoints and set them to prior mean\n",
    "known_data = arts_posterior[\"train_f\"].index.to_series()\n",
    "new_data = df[~df[data_col].isin(known_data)].index\n",
    "df.drop(new_data, inplace=True)\n",
    "new_data = pd.Series(prior_mean, index = new_data)\n",
    "\n",
    "# replace meta-category to new dummy for datapoints not in meta_arts\n",
    "new_meta_inc = df[~df[data_col].isin(meta_arts)].index\n",
    "new_meta_imp = max(dff[meta_col])+1\n",
    "df.loc[new_meta_inc, [meta_col]] = range(new_meta_imp, new_meta_imp+len(new_meta_inc))\n",
    "\n",
    "# drop datapoints with no joint meta-categoty (including dummmy)\n",
    "arts_inc = df[meta_col].drop_duplicates(keep=False).index\n",
    "df_arts = df.loc[arts_inc]\n",
    "arts_data = pd.Series(index=arts_inc)\n",
    "df.drop(arts_inc, inplace=True)\n",
    "\n",
    "# set article-numbers to integer (matching arts_ij_posterior index)\n",
    "meta_arts_inv = meta_arts.reset_index().set_index(data_col)\n",
    "df[data_col] = meta_arts_inv.loc[df[data_col]][\"index\"].to_list()\n",
    "setted = df.groupby(meta_col)[data_col].agg(\"unique\")\n",
    "basket = df.apply(lambda x: setted.loc[x[meta_col]][setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "\n",
    "\n",
    "basket_arts = basket[~basket.str.len().eq(0)].index\n",
    "basket_data = pd.Series(index=basket_arts)\n",
    "\n",
    "if type(targets) == pd.core.series.Series:\n",
    "    df_arts = pd.merge(df_arts, targets, left_index = True, right_index = True)\n",
    "    df_basketarts = pd.merge(df, targets, left_index = True, right_index = True)\n",
    "    \n",
    "setted = dff.groupby(\"BEST_NR\")[\"ARTIKELNR\"].agg(\"unique\")\n",
    "\n",
    "basket = dff.apply(lambda x: setted.loc[x[\"BEST_NR\"]][setted.loc[x[\"BEST_NR\"]]!=x[\"ARTIKELNR\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f4198",
   "metadata": {},
   "source": [
    "## Old - storing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class BayLOO2d():\n",
    "    def __init__(self, N_min = 20, variants=False):\n",
    "        self.variants = variants\n",
    "        self.N_min = N_min\n",
    "    def fit(self, data, targets, meta_group):\n",
    "        N_min = self.N_min\n",
    "        # get colnames (using pandas .name attribute -> needed atm)\n",
    "        data_col = data.name\n",
    "        target_col = targets.name\n",
    "        meta_col = meta_group.name\n",
    "\n",
    "        # generate data used for aricle-wise stage\n",
    "        prior_mean = targets.mean() # mean of dataset\n",
    "        self.prior_mean = prior_mean\n",
    "\n",
    "        df = pd.merge(data, targets, left_index = True, right_index = True)\n",
    "\n",
    "        # first stage: equal to (leave one out) beta encoder\n",
    "        stats = df.groupby(data_col)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]\n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats = stats.reset_index(level=0).set_index(data_col)\n",
    "        # generate arts_lookup for single-arts\n",
    "        stats_f = stats.copy()\n",
    "        stats_f.loc[:, \"N\"] = stats_f[\"N\"] - 1\n",
    "        # stats_f.loc[stats_f[\"N\"]>1, \"N\"] = stats_f[stats_f[\"N\"]>1][\"N\"] - 1\n",
    "        stats_t = stats_f.copy()\n",
    "        stats_t.loc[stats_t[\"n\"]>0, \"n\"] = stats_f[\"n\"] - 1\n",
    "\n",
    "        # first stage: article-meta stage for article-basket data\n",
    "        dff = pd.merge(meta_group, df, left_index = True, right_index = True)\n",
    "\n",
    "        #\n",
    "        meta_data = dff.groupby(meta_col)[data_col].apply(np.array)\n",
    "\n",
    "        meta_lens = meta_data.apply(lambda x: len(x))\n",
    "        meta_multi = meta_lens[meta_lens>1].index.to_series()\n",
    "\n",
    "        meta_arts = np.sort(dff[dff[meta_col].isin(meta_multi)][data_col].unique())\n",
    "        self.meta_arts = pd.Series(meta_arts, name=data_col)\n",
    "\n",
    "        dff = dff[dff[meta_col].isin(meta_multi)]\n",
    "        dff_rets = dff[dff[target_col]==1]\n",
    "\n",
    "        data_meta = dff.groupby(data_col)[meta_col].apply(np.array)\n",
    "        data_meta = data_meta.apply(lambda x: np.unique(x))\n",
    "        data_ret_meta = dff_rets.groupby(data_col)[meta_col].apply(np.array)\n",
    "        data_ret_meta = data_ret_meta.apply(lambda x: np.unique(x))\n",
    "\n",
    "        joint_datapoints_N = data_meta.apply(lambda x: np.concatenate(meta_data.loc[x]))\n",
    "        joint_datapoints_n = data_ret_meta.apply(lambda x: np.concatenate(meta_data.loc[x]))\n",
    "\n",
    "        counted_N = joint_datapoints_N.apply(lambda x: np.unique(x, return_counts=True))\n",
    "        counted_n = joint_datapoints_n.apply(lambda x: np.unique(x, return_counts=True))\n",
    "\n",
    "        N = counted_N.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "        n = counted_n.apply(lambda x: pd.Series(data=x[1], index=x[0]))\n",
    "        N = N[meta_arts]\n",
    "        n = n.reindex(meta_arts, axis=1).reindex(meta_arts)\n",
    "\n",
    "        if self.variants == True:\n",
    "            for art in meta_arts:\n",
    "                n.loc[art, art] = 0\n",
    "                N.loc[art,art] = 0\n",
    "            grouped = dff.groupby(data_col)[meta_col].value_counts()\n",
    "            grouped_N = grouped[grouped>1]\n",
    "            variants_N = grouped_N.groupby(data_col).sum()\n",
    "            # drops = alle arts die \n",
    "            drops = grouped[grouped==1]\n",
    "            grouped_ret = dff_rets.groupby(data_col)[meta_col].value_counts()\n",
    "            drop_inc = drops.index.get_level_values(0).unique().to_series()\n",
    "            ret_inc = grouped_ret.index.get_level_values(0).unique().to_series()\n",
    "            # drop_inc: only arts that got returned (grouped_ret)\n",
    "            drop_inc = drop_inc[drop_inc.isin(ret_inc)]\n",
    "            for inc in drop_inc:\n",
    "                # if grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore'):\n",
    "                n.loc[inc,inc] = grouped_ret.loc[inc].drop(drops.loc[inc].index, errors='ignore').sum()        \n",
    "            for item in variants_N.index:\n",
    "                N.loc[item, item] = variants_N.loc[item]\n",
    "\n",
    "            \"\"\"multi_counts = data_meta.apply(lambda x: len(x))\n",
    "            # auch bei returns\n",
    "            multi_ret_counts = data_ret_meta.apply(lambda x: len(x))\n",
    "            # abziehen von df\n",
    "            for item in multi_counts.index:\n",
    "                N.loc[item, item] = N.loc[item, item] - multi_counts.loc[item]\n",
    "            for item in multi_ret_counts.index:\n",
    "                n.loc[item, item] = n.loc[item, item] - multi_ret_counts.loc[item]\"\"\"\n",
    "        #else:\n",
    "        #    for item in meta_arts:\n",
    "        #        N.loc[item, item] = np.nan\n",
    "        #        n.loc[item, item] = np.nan\n",
    "\n",
    "        N_train = np.nan_to_num((N - 1).to_numpy(dtype=float)).astype(int)\n",
    "        N = np.nan_to_num(N.to_numpy(dtype=float)).astype(int)\n",
    "        n_train_t = np.nan_to_num((n-1).to_numpy(dtype=float)).astype(int)\n",
    "        n = np.nan_to_num(n.to_numpy(dtype=float)).astype(int)\n",
    "\n",
    "        # generate lift and confidence for sorting\n",
    "        sorting = {}\n",
    "        n_transactions = data.shape[0]\n",
    "        rel_freq_arts = (stats.loc[meta_arts, \"N\"]/n_transactions).to_numpy()\n",
    "        lift_frame = (N/n_transactions)/ (np.atleast_2d(rel_freq_arts).T * rel_freq_arts)\n",
    "        lift_frame[np.where(lift_frame==0)] = 1\n",
    "        sorting[\"lift\"] = np.argsort(lift_frame, axis=1)\n",
    "        sorting[\"conf\"] = np.argsort(N, axis=1)\n",
    "        self.sorting = sorting\n",
    "\n",
    "        frame_name = [\"train_f\", \"train_t\", \"test\"]\n",
    "\n",
    "        arts_posterior = {}\n",
    "        stats_frames = [stats_f, stats_t, stats]\n",
    "\n",
    "        arts_ij_posterior = {}\n",
    "        ns = [n,n_train_t, n]\n",
    "        Ns = [N_train, N_train, N]\n",
    "        # multi_statsframes = [multi_f_stats, multi_t_stats, multi_stats]\n",
    "        for i, name in enumerate(frame_name):\n",
    "            statsframe = stats_frames[i].copy()\n",
    "            ni = statsframe[\"n\"]\n",
    "            Ni = statsframe[\"N\"]\n",
    "            # Stufe 1: data-column based -> create stuff based on index of arts; other stuff seperately\n",
    "                # if Ni (Anzahl Artikel i) > Ni_prior: prior = 0; else N_min-AnzahlArts\n",
    "            Ni_prior = np.maximum(N_min-Ni, 0) # 1D-array\n",
    "\n",
    "            # bayes regularization for articles i: use mean_prior from dataset (no leave-one-out)\n",
    "            alpha_i_prior = prior_mean*Ni_prior\n",
    "            beta_i_prior = (1-prior_mean)*Ni_prior\n",
    "\n",
    "            alpha_i = alpha_i_prior + ni\n",
    "            beta_i = beta_i_prior + (Ni-ni)\n",
    "            values_i = alpha_i/(alpha_i+beta_i)\n",
    "\n",
    "            arts_posterior[name] = values_i\n",
    "\n",
    "                # reset index to arts (only articles that have been in orders)\n",
    "            Ni_prior = Ni_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            alpha_i_prior = alpha_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            beta_i_prior = beta_i_prior.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            values_i = values_i.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            ni = ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "            Ni = Ni.loc[meta_arts].to_numpy().reshape(len(meta_arts),-1)\n",
    "\n",
    "            # second stage: data<>meta-group combinations\n",
    "            nij = ns[i].copy()\n",
    "            Nij = Ns[i].copy()\n",
    "\n",
    "            # bayes regularization for article combinations ij\n",
    "                # leaves out evidence from existing combination ij (e.g. Ni-N)\n",
    "            nij_reg = ni-nij # number of positive observations *not* including combination nij\n",
    "            Nij_reg = Ni-Nij # number of total observations *not* including combination nij\n",
    "\n",
    "            prior_means = nij_reg/Nij_reg # prior_mean not taking combination ij into account (seperate effects) \n",
    "            inds = np.where(np.isnan(prior_means))\n",
    "            prior_means[inds] = np.take(values_i, inds[0]) # replace nas with article-wise priors\n",
    "\n",
    "            Nij_prior = np.maximum(N_min-Nij, 0) \n",
    "            Nij_prior = np.maximum(Nij_prior - Ni_prior,0) # Nij_prior >= Ni_prior\n",
    "\n",
    "            alpha_ij_prior = alpha_i_prior+prior_means*Nij_prior\n",
    "            beta_ij_prior = beta_i_prior+(1-prior_means)*Nij_prior\n",
    "\n",
    "            alpha_ij = alpha_ij_prior + nij\n",
    "            beta_ij = beta_ij_prior + (Nij-nij)\n",
    "\n",
    "            values_ij = alpha_ij/(alpha_ij+beta_ij)\n",
    "\n",
    "            arts_ij_posterior[name] = values_ij\n",
    "        self.arts_posterior = arts_posterior\n",
    "        self.arts_ij_posterior = arts_ij_posterior\n",
    "        return self # [arts_posterior, arts_ij_posterior, meta_arts, sorting, prior_mean]\n",
    "        \n",
    "    def transform(self, data, meta_group, targets=None, weighted=True, basket_max=5, sorting_type=\"lift\"):\n",
    "        \"\"\"meta_arts as indexer, since arts_ij_posterior is an array\"\"\"            \n",
    "        arts_posterior = self.arts_posterior\n",
    "        arts_ij_posterior = self.arts_ij_posterior\n",
    "        prior_mean = self.prior_mean\n",
    "        meta_arts = self.meta_arts\n",
    "        sorting = self.sorting[sorting_type]\n",
    "        \n",
    "        data_col = data.name\n",
    "        meta_col = meta_group.name\n",
    "        \n",
    "        # working with int for meta-group\n",
    "        if meta_group.dtype != int:\n",
    "            new_ind = pd.Series(index=meta_group.unique(), data = range(meta_group.nunique()))\n",
    "            meta_group = pd.Series(new_ind.loc[meta_group].to_numpy(), index = meta_group.index, name=meta_col)\n",
    "        df = pd.merge(data, meta_group, left_index = True, right_index = True)\n",
    "\n",
    "        # drop new datapoints and set them to prior mean\n",
    "            # will not use them for calculating basket_mean as well\n",
    "        known_data = arts_posterior[\"train_f\"].index.to_series()\n",
    "        new_data = df[~df[data_col].isin(known_data)].index\n",
    "        df.drop(new_data, inplace=True)\n",
    "        new_data = pd.Series(prior_mean, index = new_data)\n",
    "\n",
    "        # replace meta-category to new dummy for datapoints not in meta_arts\n",
    "        new_meta_inc = df[~df[data_col].isin(meta_arts)].index\n",
    "        new_meta_imp = max(df[meta_col])+1\n",
    "        # print(new_meta_inc, new_meta_imp)\n",
    "        df.loc[new_meta_inc, meta_col] = range(new_meta_imp, new_meta_imp+len(new_meta_inc))\n",
    "\n",
    "        # drop datapoints with no joint meta-categoty (including dummmy)\n",
    "        arts_inc = df[meta_col].drop_duplicates(keep=False).index\n",
    "        df_arts = df.loc[arts_inc]\n",
    "        arts_data = pd.Series(index=arts_inc, dtype=float)\n",
    "        df.drop(arts_inc, inplace=True)\n",
    "\n",
    "        # set categories in data to integer (matching arts_ij_posterior index)\n",
    "        meta_arts_inv = meta_arts.reset_index().set_index(data_col)\n",
    "        df[data_col] = meta_arts_inv.loc[df[data_col]][\"index\"].to_list() # \n",
    "        setted = df.groupby(meta_col)[data_col].agg(\"unique\")\n",
    "        if self.variants == True:\n",
    "            singles = df[[data_col, meta_col]].drop_duplicates(keep=False)\n",
    "            singles = singles.apply(lambda x:\n",
    "                                        setted.loc[x[meta_col]]\n",
    "                                        [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "            doubles = df[df[[data_col, meta_col]].duplicated(keep=False)].apply(lambda x:\n",
    "                                        setted.loc[x[meta_col]], axis=1)\n",
    "            data_basket = pd.concat([singles, doubles]).sort_index()\n",
    "        else:\n",
    "            data_basket = df.apply(lambda x:\n",
    "                                setted.loc[x[meta_col]]\n",
    "                                [setted.loc[x[meta_col]]!=x[data_col]], axis=1)\n",
    "        df[\"basket\"] = data_basket\n",
    "        # basket_arts = basket[~basket.str.len().eq(0)].index\n",
    "        basket_data = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "        if type(targets) == pd.core.series.Series:\n",
    "            target_col = targets.name\n",
    "            df_arts = pd.merge(df_arts, targets, left_index = True, right_index = True)\n",
    "            df = pd.merge(df, targets, left_index = True, right_index = True)\n",
    "\n",
    "            # entries in meta-group\n",
    "            # df_arts = dff.loc[arts_inc]\n",
    "            # df_basket_arts = dff.loc[basket_arts]\n",
    "            for i, trainset in enumerate(['train_f', 'train_t']):\n",
    "                arts_get = df_arts[df_arts[target_col]==i]\n",
    "                arts_lookup = arts_posterior[trainset]\n",
    "                arts_data.loc[arts_get.index] = arts_lookup.loc[arts_get[data_col]].to_list()\n",
    "\n",
    "                basket_get = df[df[target_col]==i]\n",
    "                basket_lookup = arts_ij_posterior[trainset]\n",
    "                ij_baskets = basket_get.apply(lambda x: \n",
    "                                      basket_lookup[x[data_col],\n",
    "                                                    np.argsort(sorting[x[data_col],x[\"basket\"]]\n",
    "                                                              )[-basket_max:]], axis=1)\n",
    "                if weighted == True:\n",
    "                    weights = ij_baskets.apply(lambda x: range(1, len(x)+1))\n",
    "                    weigthed = (weights * ij_baskets)\n",
    "                    avg = weigthed.apply(lambda x: sum(x)\n",
    "                                        ) / weights.apply(lambda x: sum(x))\n",
    "                else:\n",
    "                    avg = ij_baskets.apply(lambda x: x.mean())\n",
    "                basket_data.loc[basket_get.index] = avg.to_list()\n",
    "        else:\n",
    "            arts_lookup = arts_posterior['test']\n",
    "            arts_data.loc[arts_inc] = arts_lookup.loc[df_arts[data_col]].to_list()\n",
    "\n",
    "            basket_lookup = arts_ij_posterior['test']\n",
    "            ij_baskets = df.apply(lambda x: \n",
    "                                  basket_lookup[x[data_col],\n",
    "                                                np.argsort(sorting[x[data_col],x[\"basket\"]]\n",
    "                                                          )[-basket_max:]], axis=1)\n",
    "            if weighted == True:\n",
    "                weights = ij_baskets.apply(lambda x: range(1, len(x)+1))\n",
    "                weigthed = (weights * ij_baskets)\n",
    "                avg = weigthed.apply(lambda x: sum(x)\n",
    "                                    ) / weights.apply(lambda x: sum(x))\n",
    "                # print(avg.head())\n",
    "            else:\n",
    "                avg = ij_baskets.apply(lambda x: x.mean())\n",
    "                # print(avg.head())\n",
    "            basket_data.loc[avg.index] = avg.to_list()\n",
    "        values = pd.concat([new_data, arts_data, basket_data])\n",
    "        self.values_distibution = {\n",
    "            \"new\": new_data.shape[0],\n",
    "            \"arts\": arts_data.shape[0],\n",
    "            \"basket\": basket_data.shape[0]}\n",
    "        \n",
    "        return values.sort_index()\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMCFeatures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
